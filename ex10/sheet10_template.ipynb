{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSML WS24/25 Exercise Sheet 10 - Code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Sample random subsequences of length T_seq from the provided dataset.\n",
    "    The dataset is a torch tensor of shape (T, N).\"\"\"\n",
    "    def __init__(self, data, T_seq):\n",
    "        # T x N\n",
    "        self.data = data\n",
    "        self.T_seq = T_seq\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        # t is the index of the first time step\n",
    "        # return a sequence of length T_seq\n",
    "        # and the sequence shifted by one time step\n",
    "        return self.data[t:t+self.T_seq, :], self.data[t+1:t+self.T_seq+1, :]\t\n",
    "\n",
    "    def __len__(self):\n",
    "        # sets the allowed range of t\n",
    "        return len(self.data) - self.T_seq - 1\n",
    "\n",
    "\n",
    "class BatchSampler():\n",
    "    \"\"\"Samples sequences from the dataset and stacks them into batches.\"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.B = batch_size\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __call__(self):\n",
    "        # get indices\n",
    "        batch = [self.dataset[i] for i in self.get_random_inital_conditions()]\n",
    "\n",
    "        # stack the sequences into separate batches\n",
    "        xs = torch.stack([x for x, _ in batch])\n",
    "        ys = torch.stack([y for _, y in batch])\n",
    "\n",
    "        # reshape to (T, B, N)\n",
    "        return xs.permute(1, 0, 2), ys.permute(1, 0, 2)\n",
    "        \n",
    "    def get_random_inital_conditions(self):\n",
    "        # return a list of initial conditions of size self.B\n",
    "        return torch.randperm(len(self.dataset))[:self.B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.M = latent_dim\n",
    "        self.L = hidden_dim\n",
    "        self.N = output_dim\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        r1 = 1.0 / (self.L ** 0.5)\n",
    "        r2 = 1.0 / (self.M ** 0.5)\n",
    "        self.W1 = nn.Parameter(nn.init.uniform_(torch.empty(self.M, self.L), -r1, r1))\n",
    "        self.W2 = nn.Parameter(nn.init.uniform_(torch.empty(self.L, self.M), -r2, r2))\n",
    "        self.A = nn.Parameter(nn.init.uniform_(torch.empty(self.M), a=0.5, b=0.9))\n",
    "        self.h2 = nn.Parameter(nn.init.uniform_(torch.empty(self.L), -r1, r1))\n",
    "        self.h1 = nn.Parameter(torch.zeros(self.M))\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Given a vector of size (M,) or a matrix of size (B, M), compute one update of the latent state.\n",
    "        \n",
    "        Resulting shape: (M,) or (B, M)\n",
    "        \"\"\"\n",
    "        return self.A * z + torch.relu(z @ self.W2.T + self.h2) @ self.W1.T + self.h1\n",
    "\n",
    "    def __call__(self, z):\n",
    "        return self.forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_orbit(model, z0, T):\n",
    "    \"\"\"\n",
    "    Generate an orbit of `model`, i.e. starting from initial condition vector `z0`, evolve the system for `T` time steps.\n",
    "    \n",
    "    Returns:\n",
    "        orbit: torch.Tensor of shape (T, M)\n",
    "    \"\"\"\n",
    "    M = len(z0)\n",
    "    orbit = torch.empty(T, M)\n",
    "    orbit[0, :] = z0\n",
    "    for t in range(1, T):\n",
    "        orbit[t, :] = model(orbit[t-1, :])\n",
    "    return orbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Sparse Teacher Forcing for training RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Complete the code to train the `CustomRNN` using sparse teacher forcing (STF). Everywhere you see\n",
    "```python\n",
    "... # your code here\n",
    "```\n",
    "you are supposed to implement the corresponding functionality that is explained at that location in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence_using_STF(model, x, t_forcing):\n",
    "    \"\"\"\n",
    "    Performs an entire forward pass of the model given training sequence `x`\n",
    "    using sparse teacher forcing with forcing interval `t_forcing`.\n",
    "    \"\"\"\n",
    "    T, B, N = x.shape\n",
    "    \n",
    "    # manually initialize the hidden state of the model with zeros\n",
    "    z0 = ... # your code here\n",
    "    \n",
    "    # force the read-out neurons of the rnn with the first data point in the sequence\n",
    "    z0[:, :N] = ... # your code here\n",
    "\n",
    "    # will hold the entire predicted latent sequence\n",
    "    Z = torch.zeros(T, B, model.M)\n",
    "    \n",
    "    # initial prediction based on first data point\n",
    "    z = model(z0)\n",
    "    Z[0, :, :] = z\n",
    "    \n",
    "    # remaining forward pass using STF\n",
    "    for t in range(1, T):\n",
    "        # intervals of the forcing signal\n",
    "        if t % t_forcing == 0:\n",
    "            # force the read-out neurons of the rnn\n",
    "            z[:, :N] = ... # your code here\n",
    "    \n",
    "        # update the (sparsely) forced latent state using the model\n",
    "        z = ... # your code here\n",
    "        \n",
    "        # store the predicted latent state\n",
    "        Z[t, :, :] = z\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(\n",
    "    rnn,\n",
    "    dataloader, \n",
    "    t_forcing,\n",
    "    n_epochs, \n",
    "    print_every,\n",
    "    lr=5e-4\n",
    "):  \n",
    "    # gather parameters\n",
    "    rnn_params = list(rnn.parameters())\n",
    "\n",
    "    # the optimizer performing stochastic gradient descent\n",
    "    optimizer = torch.optim.Adam(rnn_params, lr=lr)\n",
    "\n",
    "    # the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        # get the data\n",
    "        xs, ys = dataloader()\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass of the entire batch\n",
    "        Z = process_sequence_using_STF(rnn, xs, t_forcing)\n",
    "        \n",
    "        # output layer simply returns the read-out neurons' outputs\n",
    "        y_pred = ... # your code here\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = criterion(y_pred, ys)\n",
    "\n",
    "        # backward pass, computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # store the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # print the loss\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train the RNN and visually inspect the dynamics of the model\n",
    "The hyperparameter settings should at least give you chaotic behavior, but in general you will get better models by increasing the number of epochs (if you have time and resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyper parameters\n",
    "T_seq = 50\n",
    "B = 16\n",
    "epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "t_forcing = 16\n",
    "M = 10\n",
    "L = 50\n",
    "\n",
    "# load the data\n",
    "X = torch.load('lorenz_data.pt')\n",
    "\n",
    "# initialize the dataset\n",
    "dataset = CustomDataset(X, T_seq)\n",
    "\n",
    "# initialize the dataloader\n",
    "dataloader = BatchSampler(dataset, B)\n",
    "xs, ys = dataloader()\n",
    "\n",
    "# initialize the model\n",
    "rnn = CustomRNN(M, L, X.shape[-1])\n",
    "\n",
    "# train the model\n",
    "losses = train_RNN(rnn, dataloader, t_forcing=t_forcing, n_epochs=epochs, print_every=int(epochs/10), lr=learning_rate)\n",
    "\n",
    "# optionally save the trained model\n",
    "#torch.save(rnn.state_dict(), \"pretrained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a trajectory with your model using the generate_orbit function\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3D state space\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Maximum Lyapunov Exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following code box if you want to use the pretrained model instead of your own for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = CustomRNN(10, 50, 3)\n",
    "# rnn.load_state_dict(torch.load(\"pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "(Your answer to question Task 2.1 here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_max_lyapunov_exponent(model, z1, T):\n",
    "    \"\"\"\n",
    "    Compute the maximum Lyapunov exponent for a orbit starting from `z1` of length `T`.\n",
    "    \"\"\"\n",
    "    pass # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct the experiment of computing the maximum Lyapunov exponent naively\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def maximum_lyapunov_exponent(model, z1, T, T_trans=1000):\n",
    "    \"\"\"\n",
    "    Compute the maximum Lyapunov exponent for a orbit starting from `z1` of length `T`.\n",
    "    \n",
    "    The orbit is first evolved for `T_trans` time steps to get rid of the transient.\n",
    "    Uses reorthogonalization to keep Jacobian product numerically stable.\n",
    "    \"\"\"\n",
    "\n",
    "    # evolve for transient time T_trans\n",
    "    Z = generate_orbit(model, z1, T_trans)\n",
    "\n",
    "    # initialize\n",
    "    z = Z[-1]\n",
    "    \n",
    "    # max lyap init\n",
    "    lyap = 0\n",
    "    \n",
    "    # initialize as Identity matrix\n",
    "    Q = torch.eye(model.M)\n",
    "\n",
    "    for t in range(T):\n",
    "        z = model(z)\n",
    "        J = torch.autograd.functional.jacobian(model, z)\n",
    "        Q = J @ Q\n",
    "\n",
    "        # reorthogonalize\n",
    "        Q, R = torch.linalg.qr(Q)\n",
    "\n",
    "        # accumulate lyapunov exponents\n",
    "        lyap += torch.log(torch.abs(R[0, 0])).item()\n",
    "\n",
    "    return lyap / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the maximum Lyapunov exponent using the reorthogonalization method\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the relative absolute error compared to the ground truth exponent\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
