{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSML WS24/25 Exercise Sheet 10 - Code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Sample random subsequences of length T_seq from the provided dataset.\n",
    "    The dataset is a torch tensor of shape (T, N).\"\"\"\n",
    "    def __init__(self, data, T_seq):\n",
    "        # T x N\n",
    "        self.data = data\n",
    "        self.T_seq = T_seq\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        # t is the index of the first time step\n",
    "        # return a sequence of length T_seq\n",
    "        # and the sequence shifted by one time step\n",
    "        return self.data[t:t+self.T_seq, :], self.data[t+1:t+self.T_seq+1, :]\t\n",
    "\n",
    "    def __len__(self):\n",
    "        # sets the allowed range of t\n",
    "        return len(self.data) - self.T_seq - 1\n",
    "\n",
    "\n",
    "class BatchSampler():\n",
    "    \"\"\"Samples sequences from the dataset and stacks them into batches.\"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.B = batch_size\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __call__(self):\n",
    "        # get indices\n",
    "        batch = [self.dataset[i] for i in self.get_random_inital_conditions()]\n",
    "\n",
    "        # stack the sequences into separate batches\n",
    "        xs = torch.stack([x for x, _ in batch])\n",
    "        ys = torch.stack([y for _, y in batch])\n",
    "\n",
    "        # reshape to (T, B, N)\n",
    "        return xs.permute(1, 0, 2), ys.permute(1, 0, 2)\n",
    "        \n",
    "    def get_random_inital_conditions(self):\n",
    "        # return a list of initial conditions of size self.B\n",
    "        return torch.randperm(len(self.dataset))[:self.B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.M = latent_dim\n",
    "        self.L = hidden_dim\n",
    "        self.N = output_dim\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        r1 = 1.0 / (self.L ** 0.5)\n",
    "        r2 = 1.0 / (self.M ** 0.5)\n",
    "        self.W1 = nn.Parameter(nn.init.uniform_(torch.empty(self.M, self.L), -r1, r1))\n",
    "        self.W2 = nn.Parameter(nn.init.uniform_(torch.empty(self.L, self.M), -r2, r2))\n",
    "        self.A = nn.Parameter(nn.init.uniform_(torch.empty(self.M), a=0.5, b=0.9))\n",
    "        self.h2 = nn.Parameter(nn.init.uniform_(torch.empty(self.L), -r1, r1))\n",
    "        self.h1 = nn.Parameter(torch.zeros(self.M))\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Given a vector of size (M,) or a matrix of size (B, M), compute one update of the latent state.\n",
    "        \n",
    "        Resulting shape: (M,) or (B, M)\n",
    "        \"\"\"\n",
    "        return self.A * z + torch.relu(z @ self.W2.T + self.h2) @ self.W1.T + self.h1\n",
    "\n",
    "    def __call__(self, z):\n",
    "        return self.forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_orbit(model, z0, T):\n",
    "    \"\"\"\n",
    "    Generate an orbit of `model`, i.e. starting from initial condition vector `z0`, evolve the system for `T` time steps.\n",
    "    \n",
    "    Returns:\n",
    "        orbit: torch.Tensor of shape (T, M)\n",
    "    \"\"\"\n",
    "    M = len(z0)\n",
    "    orbit = torch.empty(T, M)\n",
    "    orbit[0, :] = z0\n",
    "    for t in range(1, T):\n",
    "        orbit[t, :] = model(orbit[t-1, :])\n",
    "    return orbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Sparse Teacher Forcing for training RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Complete the code to train the `CustomRNN` using sparse teacher forcing (STF). Everywhere you see\n",
    "```python\n",
    "... # your code here\n",
    "```\n",
    "you are supposed to implement the corresponding functionality that is explained at that location in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence_using_STF(model, x, t_forcing):\n",
    "    \"\"\"\n",
    "    Performs an entire forward pass of the model given training sequence `x`\n",
    "    using sparse teacher forcing with forcing interval `t_forcing`.\n",
    "    \"\"\"\n",
    "    T, B, N = x.shape\n",
    "    \n",
    "    # manually initialize the hidden state of the model with zeros\n",
    "    z0 = torch.zeros((B, model.L)) # your code here\n",
    "    \n",
    "    # force the read-out neurons of the rnn with the first data point in the sequence\n",
    "    z0[:, :N] = x[0,:,:] # your code here\n",
    "\n",
    "    # will hold the entire predicted latent sequence\n",
    "    Z = torch.zeros(T, B, model.M)\n",
    "    \n",
    "    # initial prediction based on first data point\n",
    "    z = model(z0)\n",
    "    Z[0, :, :] = z\n",
    "    \n",
    "    # remaining forward pass using STF\n",
    "    for t in range(1, T):\n",
    "        # intervals of the forcing signal\n",
    "        if t % t_forcing == 0:\n",
    "            # force the read-out neurons of the rnn\n",
    "            z[:, :N] = x[t,:,:] # your code here\n",
    "    \n",
    "        # update the (sparsely) forced latent state using the model\n",
    "        z = model(z) # your code here\n",
    "        \n",
    "        # store the predicted latent state\n",
    "        Z[t, :, :] = z\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(\n",
    "    rnn,\n",
    "    dataloader, \n",
    "    t_forcing,\n",
    "    n_epochs, \n",
    "    print_every,\n",
    "    lr=5e-4\n",
    "):  \n",
    "    # gather parameters\n",
    "    rnn_params = list(rnn.parameters())\n",
    "\n",
    "    # the optimizer performing stochastic gradient descent\n",
    "    optimizer = torch.optim.Adam(rnn_params, lr=lr)\n",
    "\n",
    "    # the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        # get the data\n",
    "        xs, ys = dataloader()\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass of the entire batch\n",
    "        Z = process_sequence_using_STF(rnn, xs, t_forcing)\n",
    "        \n",
    "        # output layer simply returns the read-out neurons' outputs\n",
    "        y_pred = Z[:,:N] # your code here\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = criterion(y_pred, ys)\n",
    "\n",
    "        # backward pass, computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # store the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # print the loss\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train the RNN and visually inspect the dynamics of the model\n",
    "The hyperparameter settings should at least give you chaotic behavior, but in general you will get better models by increasing the number of epochs (if you have time and resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1511522/3197728476.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load('lorenz_data.pt')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m rnn \u001b[38;5;241m=\u001b[39m CustomRNN(M, L, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_RNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_forcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_forcing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# optionally save the trained model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#torch.save(rnn.state_dict(), \"pretrained_model.pt\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mtrain_RNN\u001b[0;34m(rnn, dataloader, t_forcing, n_epochs, print_every, lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# forward pass of the entire batch\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sequence_using_STF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_forcing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# output layer simply returns the read-out neurons' outputs\u001b[39;00m\n\u001b[1;32m     30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m Z[:,:N] \u001b[38;5;66;03m# your code here\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mprocess_sequence_using_STF\u001b[0;34m(model, x, t_forcing)\u001b[0m\n\u001b[1;32m      9\u001b[0m z0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((B, model\u001b[38;5;241m.\u001b[39mL)) \u001b[38;5;66;03m# your code here\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# force the read-out neurons of the rnn with the first data point in the sequence\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m z0[:, :N] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:,:N] \u001b[38;5;66;03m# your code here\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# will hold the entire predicted latent sequence\u001b[39;00m\n\u001b[1;32m     15\u001b[0m Z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(T, B, model\u001b[38;5;241m.\u001b[39mM)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mCustomRNN.__call__\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mCustomRNN.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Given a vector of size (M,) or a matrix of size (B, M), compute one update of the latent state.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    Resulting shape: (M,) or (B, M)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(z \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh2) \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh1\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# set the hyper parameters\n",
    "T_seq = 50\n",
    "B = 16\n",
    "epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "t_forcing = 16\n",
    "M = 10\n",
    "L = 50\n",
    "\n",
    "# load the data\n",
    "X = torch.load('lorenz_data.pt')\n",
    "\n",
    "# initialize the dataset\n",
    "dataset = CustomDataset(X, T_seq)\n",
    "\n",
    "# initialize the dataloader\n",
    "dataloader = BatchSampler(dataset, B)\n",
    "xs, ys = dataloader()\n",
    "\n",
    "# initialize the model\n",
    "rnn = CustomRNN(M, L, X.shape[-1])\n",
    "\n",
    "# train the model\n",
    "losses = train_RNN(rnn, dataloader, t_forcing=t_forcing, n_epochs=epochs, print_every=int(epochs/10), lr=learning_rate)\n",
    "\n",
    "# optionally save the trained model\n",
    "#torch.save(rnn.state_dict(), \"pretrained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a trajectory with your model using the generate_orbit function\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3D state space\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Maximum Lyapunov Exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following code box if you want to use the pretrained model instead of your own for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = CustomRNN(10, 50, 3)\n",
    "# rnn.load_state_dict(torch.load(\"pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "(Your answer to question Task 2.1 here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_max_lyapunov_exponent(model, z1, T):\n",
    "    \"\"\"\n",
    "    Compute the maximum Lyapunov exponent for a orbit starting from `z1` of length `T`.\n",
    "    \"\"\"\n",
    "    pass # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct the experiment of computing the maximum Lyapunov exponent naively\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def maximum_lyapunov_exponent(model, z1, T, T_trans=1000):\n",
    "    \"\"\"\n",
    "    Compute the maximum Lyapunov exponent for a orbit starting from `z1` of length `T`.\n",
    "    \n",
    "    The orbit is first evolved for `T_trans` time steps to get rid of the transient.\n",
    "    Uses reorthogonalization to keep Jacobian product numerically stable.\n",
    "    \"\"\"\n",
    "\n",
    "    # evolve for transient time T_trans\n",
    "    Z = generate_orbit(model, z1, T_trans)\n",
    "\n",
    "    # initialize\n",
    "    z = Z[-1]\n",
    "    \n",
    "    # max lyap init\n",
    "    lyap = 0\n",
    "    \n",
    "    # initialize as Identity matrix\n",
    "    Q = torch.eye(model.M)\n",
    "\n",
    "    for t in range(T):\n",
    "        z = model(z)\n",
    "        J = torch.autograd.functional.jacobian(model, z)\n",
    "        Q = J @ Q\n",
    "\n",
    "        # reorthogonalize\n",
    "        Q, R = torch.linalg.qr(Q)\n",
    "\n",
    "        # accumulate lyapunov exponents\n",
    "        lyap += torch.log(torch.abs(R[0, 0])).item()\n",
    "\n",
    "    return lyap / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the maximum Lyapunov exponent using the reorthogonalization method\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the relative absolute error compared to the ground truth exponent\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
