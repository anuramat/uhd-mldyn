{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ca79d7",
   "metadata": {},
   "source": [
    "# DSML WS24/25 Exercise Sheet 10 - Code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ce014d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a10fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a351ef5",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Sample random subsequences of length T_seq from the provided dataset.\n",
    "    The dataset is a torch tensor of shape (T, N).\"\"\"\n",
    "\n",
    "    def __init__(self, data, T_seq):\n",
    "        # T x N\n",
    "        self.data = data\n",
    "        self.T_seq = T_seq\n",
    "\n",
    "    def __getitem__(self, t):\n",
    "        # t is the index of the first time step\n",
    "        # return a sequence of length T_seq\n",
    "        # and the sequence shifted by one time step\n",
    "        return (\n",
    "            self.data[t : t + self.T_seq, :],\n",
    "            self.data[t + 1 : t + self.T_seq + 1, :],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # sets the allowed range of t\n",
    "        return len(self.data) - self.T_seq - 1\n",
    "\n",
    "\n",
    "class BatchSampler:\n",
    "    \"\"\"Samples sequences from the dataset and stacks them into batches.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.B = batch_size\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __call__(self):\n",
    "        # get indices\n",
    "        batch = [self.dataset[i] for i in self.get_random_inital_conditions()]\n",
    "\n",
    "        # stack the sequences into separate batches\n",
    "        xs = torch.stack([x for x, _ in batch])\n",
    "        ys = torch.stack([y for _, y in batch])\n",
    "\n",
    "        # reshape to (T, B, N)\n",
    "        return xs.permute(1, 0, 2), ys.permute(1, 0, 2)\n",
    "\n",
    "    def get_random_inital_conditions(self):\n",
    "        # return a list of initial conditions of size self.B\n",
    "        return torch.randperm(len(self.dataset))[: self.B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76e41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.M = latent_dim\n",
    "        self.L = hidden_dim\n",
    "        self.N = output_dim\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        r1 = 1.0 / (self.L**0.5)\n",
    "        r2 = 1.0 / (self.M**0.5)\n",
    "        self.W1 = nn.Parameter(nn.init.uniform_(torch.empty(self.M, self.L), -r1, r1))\n",
    "        self.W2 = nn.Parameter(nn.init.uniform_(torch.empty(self.L, self.M), -r2, r2))\n",
    "        self.A = nn.Parameter(nn.init.uniform_(torch.empty(self.M), a=0.5, b=0.9))\n",
    "        self.h2 = nn.Parameter(nn.init.uniform_(torch.empty(self.L), -r1, r1))\n",
    "        self.h1 = nn.Parameter(torch.zeros(self.M))\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Given a vector of size (M,) or a matrix of size (B, M), compute one update of the latent state.\n",
    "\n",
    "        Resulting shape: (M,) or (B, M)\n",
    "        \"\"\"\n",
    "        return self.A * z + torch.relu(z @ self.W2.T + self.h2) @ self.W1.T + self.h1\n",
    "\n",
    "    def __call__(self, z):\n",
    "        return self.forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905864d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_orbit(model, z0, T):\n",
    "    \"\"\"\n",
    "    Generate an orbit of `model`, i.e. starting from initial condition vector `z0`, evolve the system for `T` time steps.\n",
    "\n",
    "    Returns:\n",
    "        orbit: torch.Tensor of shape (T, M)\n",
    "    \"\"\"\n",
    "    M = len(z0)\n",
    "    orbit = torch.empty(T, M)\n",
    "    orbit[0, :] = z0\n",
    "    for t in range(1, T):\n",
    "        orbit[t, :] = model(orbit[t - 1, :])\n",
    "    return orbit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90355066",
   "metadata": {},
   "source": [
    "## 1 Sparse Teacher Forcing for training RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e017d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "1.1 Complete the code to train the `CustomRNN` using sparse teacher forcing (STF). Everywhere you see\n",
    "```python\n",
    "... # your code here\n",
    "```\n",
    "you are supposed to implement the corresponding functionality that is explained at that location in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f40cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence_using_STF(model, x, t_forcing):\n",
    "    \"\"\"\n",
    "    Performs an entire forward pass of the model given training sequence `x`\n",
    "    using sparse teacher forcing with forcing interval `t_forcing`.\n",
    "    \"\"\"\n",
    "    T, B, N = x.shape\n",
    "\n",
    "    # manually initialize the hidden state of the model with zeros\n",
    "    z0 = torch.zeros((B, model.M))  # your code here\n",
    "\n",
    "    # force the read-out neurons of the rnn with the first data point in the sequence\n",
    "    z0[:, :N] = x[0, :, :]  # your code here\n",
    "\n",
    "    # will hold the entire predicted latent sequence\n",
    "    Z = torch.zeros(T, B, model.M)\n",
    "\n",
    "    # initial prediction based on first data point\n",
    "    z = model(z0)\n",
    "    Z[0, :, :] = z\n",
    "\n",
    "    # remaining forward pass using STF\n",
    "    for t in range(1, T):\n",
    "        # intervals of the forcing signal\n",
    "        if t % t_forcing == 0:\n",
    "            # force the read-out neurons of the rnn\n",
    "            z[:, :N] = x[t, :, :]  # your code here\n",
    "\n",
    "        # update the (sparsely) forced latent state using the model\n",
    "        z = model(z)  # your code here\n",
    "\n",
    "        # store the predicted latent state\n",
    "        Z[t, :, :] = z\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20795ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(rnn, dataloader, t_forcing, n_epochs, print_every, lr=5e-4):\n",
    "    # gather parameters\n",
    "    rnn_params = list(rnn.parameters())\n",
    "\n",
    "    # the optimizer performing stochastic gradient descent\n",
    "    optimizer = torch.optim.Adam(rnn_params, lr=lr)\n",
    "\n",
    "    # the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        # get the data\n",
    "        xs, ys = dataloader()\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass of the entire batch\n",
    "        Z = process_sequence_using_STF(rnn, xs, t_forcing)\n",
    "\n",
    "        # output layer simply returns the read-out neurons' outputs\n",
    "        N = xs.shape[-1]\n",
    "        y_pred = Z[:, :, :N]  # your code here\n",
    "\n",
    "        # compute the loss\n",
    "        loss = criterion(y_pred, ys)\n",
    "\n",
    "        # backward pass, computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # store the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # print the loss\n",
    "        if epoch % print_every == 0:\n",
    "            print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e06d96",
   "metadata": {},
   "source": [
    "### 2.2 Train the RNN and visually inspect the dynamics of the model\n",
    "The hyperparameter settings should at least give you chaotic behavior, but in general you will get better models by increasing the number of epochs (if you have time and resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a7cc82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1511522/755594620.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X = torch.load(\"lorenz_data.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8361557722091675\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m rnn \u001b[38;5;241m=\u001b[39m CustomRNN(M, L, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_RNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_forcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_forcing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# optionally save the trained model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# torch.save(rnn.state_dict(), \"pretrained_model.pt\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mtrain_RNN\u001b[0;34m(rnn, dataloader, t_forcing, n_epochs, print_every, lr)\u001b[0m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, ys)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# backward pass, computes gradients\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# update the parameters\u001b[39;00m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/nix/store/kpk3z09fmpx3rlx0m9q5p7h4j9hbn860-python3-3.12.7-env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/kpk3z09fmpx3rlx0m9q5p7h4j9hbn860-python3-3.12.7-env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/kpk3z09fmpx3rlx0m9q5p7h4j9hbn860-python3-3.12.7-env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set the hyper parameters\n",
    "T_seq = 50\n",
    "B = 16\n",
    "epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "t_forcing = 16\n",
    "M = 10\n",
    "L = 50\n",
    "\n",
    "# load the data\n",
    "X = torch.load(\"lorenz_data.pt\")\n",
    "\n",
    "# initialize the dataset\n",
    "dataset = CustomDataset(X, T_seq)\n",
    "\n",
    "# initialize the dataloader\n",
    "dataloader = BatchSampler(dataset, B)\n",
    "xs, ys = dataloader()\n",
    "\n",
    "# initialize the model\n",
    "rnn = CustomRNN(M, L, X.shape[-1])\n",
    "\n",
    "# train the model\n",
    "losses = train_RNN(\n",
    "    rnn,\n",
    "    dataloader,\n",
    "    t_forcing=t_forcing,\n",
    "    n_epochs=epochs,\n",
    "    print_every=int(epochs / 10),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "# optionally save the trained model\n",
    "# torch.save(rnn.state_dict(), \"pretrained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a trajectory with your model using the generate_orbit function\n",
    "...  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efffe06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# plot 3D state space\n",
    "...  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50396617",
   "metadata": {},
   "source": [
    "### 2 Maximum Lyapunov Exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00956e5",
   "metadata": {},
   "source": [
    "Uncomment the following code box if you want to use the pretrained model instead of your own for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e06140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = CustomRNN(10, 50, 3)\n",
    "# rnn.load_state_dict(torch.load(\"pretrained_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47efd69",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "(Your answer to question Task 2.1 here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac748a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_max_lyapunov_exponent(model, z1, T):\n",
    "    \"\"\"\n",
    "    Compute the maximum Lyapunov exponent for a orbit starting from `z1` of length `T`.\n",
    "    \"\"\"\n",
    "    pass  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfcdf0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# conduct the experiment of computing the maximum Lyapunov exponent naively\n",
    "...  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc753f3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d870a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def maximum_lyapunov_exponent(model, z1, T, T_trans=1000):\n",
    "    \"\"\"\n",
    "    Compute the maximum Lyapunov exponent for a orbit starting from `z1` of length `T`.\n",
    "\n",
    "    The orbit is first evolved for `T_trans` time steps to get rid of the transient.\n",
    "    Uses reorthogonalization to keep Jacobian product numerically stable.\n",
    "    \"\"\"\n",
    "\n",
    "    # evolve for transient time T_trans\n",
    "    Z = generate_orbit(model, z1, T_trans)\n",
    "\n",
    "    # initialize\n",
    "    z = Z[-1]\n",
    "\n",
    "    # max lyap init\n",
    "    lyap = 0\n",
    "\n",
    "    # initialize as Identity matrix\n",
    "    Q = torch.eye(model.M)\n",
    "\n",
    "    for t in range(T):\n",
    "        z = model(z)\n",
    "        J = torch.autograd.functional.jacobian(model, z)\n",
    "        Q = J @ Q\n",
    "\n",
    "        # reorthogonalize\n",
    "        Q, R = torch.linalg.qr(Q)\n",
    "\n",
    "        # accumulate lyapunov exponents\n",
    "        lyap += torch.log(torch.abs(R[0, 0])).item()\n",
    "\n",
    "    return lyap / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8865e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the maximum Lyapunov exponent using the reorthogonalization method\n",
    "...  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3393a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the relative absolute error compared to the ground truth exponent\n",
    "...  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615a95f",
   "metadata": {},
   "source": [
    "Discuss the results here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
